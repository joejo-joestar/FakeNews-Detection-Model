{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📰 Fake News Detection Using Ensemble Learning\n",
    "\n",
    "---\n",
    "\n",
    "| S. No |    ID No.     | Name             |\n",
    "| ----: | :-----------: | :--------------- |\n",
    "|    1. | 2022A7PS0003U | Yusra Hakim      |\n",
    "|    2. | 2022A7PS0019U | Joseph Cijo      |\n",
    "|    3. | 2022A7PS0031U | Ritvik Bhatnagar |\n",
    "\n",
    "This Jupyter Notebook is for the project in Data Mining (CS F415) course. It contains the code used for preparing the ensemble model using the [dataset](#dataset-used) mentioned below.\n",
    "\n",
    "### Dataset Details\n",
    "\n",
    "**Dataset introduced in:**\n",
    "V. Pawan Kumar, A. Prateek, A. Ivone and P. Radu, \"WELFake: Word Embedding Over Linguistic Features for Fake News Detection,\" _IEEE Transactions on Computational Social Systems_, vol. 8, no. 4, pp. 881-893, 2021.\n",
    "[Kaggle | WELFake Dataset](https://www.kaggle.com/datasets/saurabhshahane/fake-news-classification)\n",
    "\n",
    "### Setting Up an Environment for the Jupyter notebook\n",
    "\n",
    "Firstly install Miniconda from [here](https://docs.anaconda.com/miniconda/install/).\n",
    "\n",
    "Then open a command prompt in this directory, and run the following. This will create and activate an environment called \"PROJ\".\n",
    "\n",
    "```bash\n",
    "    conda create -n proj python=3.12\n",
    "    conda activate proj\n",
    "```\n",
    "\n",
    "After running this, your CMD prompt should have a \"`(proj)`\" prefixed at the start.\n",
    "\n",
    "Run the following command to install packages, such as [PyTorch](https://pytorch.org/get-started/locally/). This will take some time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%conda install -n proj ipykernel ipywidgets --update-deps --force-reinstall\n",
    "%conda install -n proj nltk\n",
    "%conda install -n proj conda-forge::textblob\n",
    "%pip install scikit-learn matplotlib pandas pyperclip contractions scipy numpy\n",
    "%conda install -n proj conda-forge::transformers\n",
    "%pip install torch --index-url https://download.pytorch.org/whl/cu124\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to select the PROJ environment at the bottom-right.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 📚 Outline\n",
    "\n",
    "To train the model, the following steps must be followed:\n",
    "\n",
    "<img src=\"assets/archdiag.png\" size=64/>\n",
    "\n",
    "The model will accept two inputs:\n",
    "1. The headline of the article\n",
    "2. Content of the article (Optional)\n",
    "\n",
    "And give an output if it is fake news or factual (real) news.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 📑 Dataset Tomfoolery\n",
    "\n",
    "The dataset is a CSV file with the following columns:\n",
    "\n",
    "- _title_: the title of the article\n",
    "- _text_: the text of the article\n",
    "- _label_: the label of the article (0 for fake, 1 for real)\n",
    "\n",
    "The dataset used is the WELFake dataset [available here](https://www.kaggle.com/datasets/saurabhshahane/fake-news-classification)\n",
    "\n",
    "This dataset is a collection of news articles, with each article labeled as either fake or real. The dataset contains 10,000 articles, with 5,000 fake and 5,000 real articles. The articles are in English and cover a wide range of topics, including politics, sports, entertainment, and technology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import random\n",
    "import re\n",
    "import contractions\n",
    "\n",
    "from transformers.models.distilbert import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "import torch\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset_and_corpora/WELFake_Dataset.csv\", index_col=0)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✂️ Splitting Dataset\n",
    "\n",
    "The dataset is split as\n",
    "-  80% → Training + Validation Set\n",
    "-  20% → Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training set size: {train_df.shape}\")\n",
    "train_df.to_csv(\"dataset_and_corpora/train.csv\", index=False)\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nTesting set size: {test_df.shape}\")\n",
    "test_df.to_csv(\"dataset_and_corpora/test.csv\", index=False)\n",
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 👾 Data Preprocessing\n",
    "\n",
    "The data is preprocessed using the following steps\n",
    "1. [Text Cleaning](#text-cleaning)\n",
    "   1. [Lowercasing & URL Removal](#lowercasing-url-removal)\n",
    "   2. [Contractions Expansion](#contractions-expansion)\n",
    "   3. [Tokenization](#tokenization)\n",
    "   4. [Lemmatization](#lemmatization)\n",
    "   5. [Stopword Removal](#stopword-removal)\n",
    "2. [Data Augmentaiton](#data-augmentation)\n",
    "   1. [Synonym Replacement](#synonym-replacement)\n",
    "   2. [Random Insertion](#random-insertion)\n",
    "   3. [Random Swap](#random-swap)\n",
    "   4. [Random Deletion](#random-deletion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧹 Data Cleaning\n",
    "\n",
    "The text in the datast has to be cleaned before it can be used for training the model. The following steps are used for cleaning the text.\n",
    "\n",
    "[Kaggle | Getting started with Text Preprocessing](https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging Text Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_title_and_text(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge the 'title' and 'text' fields into a single 'content' field.\n",
    "\n",
    "    :param df: Input DataFrame\n",
    "    :return: DataFrame with a new 'content' field\n",
    "    \"\"\"\n",
    "\n",
    "    df[\"content\"] = df[\"title\"].fillna(\"\") + \" \" + df[\"text\"].fillna(\"\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Handle missing data in the DataFrame by removing rows with any empty, NaN values, or rows containing only whitespaces.\n",
    "\n",
    "    :param df: DataFrame to process\n",
    "    :return: DataFrame with rows containing any empty, NaN values, or whitespaces-only content removed\n",
    "    \"\"\"\n",
    "\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(\"Missing values in each column:\\n\", missing_values)\n",
    "\n",
    "    df = df.replace(\"\", np.nan)\n",
    "\n",
    "    df = df.dropna()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowercasing & URL Removal\n",
    "\n",
    "The text is converted to lowercase and URLs are removed from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_and_remove_urls(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert text to lowercase and remove URLs.\n",
    "\n",
    "    :param text: Input text\n",
    "    :return: Processed text with URLs removed\n",
    "    \"\"\"\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contractions Expansion\n",
    "\n",
    "Contractions are expanded to their full form. For example, \"I'm\" is expanded to \"I am\". Using the [`contractions`](https://github.com/kootenpv/contractions/tree/master) library for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Expand contractions in the text using the `contractions` library.\n",
    "\n",
    "    :param text: Input text\n",
    "    :return: Text with contractions expanded\n",
    "    \"\"\"\n",
    "\n",
    "    expanded_words = [contractions.fix(word) for word in text.split()]\n",
    "    expanded_text = \" \".join(expanded_words)  # type: ignore\n",
    "    return expanded_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "\n",
    "The text is tokenized into words. This also removes punctuation and any special characters.\n",
    "\n",
    "The [`RegexpTokenizer`](https://www.nltk.org/api/nltk.tokenize.RegexpTokenizer.html) tokenizer is used for tokenization by aplphanumeric words and to ignore all other characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Tokenize the input text using NLTK's RegexpTokenizer.\n",
    "\n",
    "    :param text: Input text\n",
    "    :return: List of tokens\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "\n",
    "The words are lemmatized to their root form. This is done using the [`WordNetLemmatizer`](https://www.nltk.org/api/nltk.stem.html#nltk.stem.wordnet.WordNetLemmatizer) from the NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "nltk.download(\"wordnet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def nltkToWordnet(nltk_tag: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert NLTK POS tags to WordNet POS tags.\n",
    "\n",
    "    :param nltk_tag: NLTK POS tag\n",
    "    :return: WordNet POS tag\n",
    "    \"\"\"\n",
    "\n",
    "    if nltk_tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None  # type: ignore\n",
    "\n",
    "\n",
    "def lemmatize_text(tokens: list) -> list:\n",
    "    \"\"\"\n",
    "    Lemmatize the tokens using NLTK's WordNetLemmatizer.\n",
    "\n",
    "    :param tokens: List of tokens\n",
    "    :return: List of lemmatized words\n",
    "    \"\"\"\n",
    "\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    res_words = []\n",
    "    for word, tag in pos_tags:\n",
    "        tag = nltkToWordnet(tag)\n",
    "        if tag is None:\n",
    "            res_words.append(word)\n",
    "        else:\n",
    "            res_words.append(lemmatizer.lemmatize(word, tag))\n",
    "    return res_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopword Removal\n",
    "\n",
    "Stopwords are removed from the text. Stopwords are common words that do not add much meaning to the text. The [`stopwords`](https://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.wordlist.WordListCorpusReader) from the NLTK library are used for this purpose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"words\")\n",
    "nltk.download(\"stopwords\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "englishWords = set(nltk.corpus.words.words())\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens: list) -> str:\n",
    "    \"\"\"\n",
    "    Remove stop words from the list of tokens.\n",
    "\n",
    "    :param tokens: List of tokens\n",
    "    :return: List of tokens without stop words\n",
    "    \"\"\"\n",
    "\n",
    "    filtered_words = [w for w in tokens if (w in englishWords and w not in stop_words)]\n",
    "    filtered_text = \" \".join(filtered_words)\n",
    "    return filtered_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing Text Cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to perform text cleaning on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean the input text by applying various preprocessing steps.\n",
    "\n",
    "    :param text: Input text\n",
    "    :return: Cleaned text\n",
    "    \"\"\"\n",
    "    text = lowercase_and_remove_urls(text)\n",
    "    text = expand_contractions(text)\n",
    "    tokens = tokenize_text(text)\n",
    "    tokens = lemmatize_text(tokens)\n",
    "    text = remove_stopwords(tokens)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress = widgets.IntProgress(\n",
    "    value=0, min=0, max=len(train_df), description=\"Progress:\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(progress)\n",
    "\n",
    "cleaned_data = []\n",
    "train_df = merge_title_and_text(train_df)\n",
    "train_df = handle_missing_data(train_df)\n",
    "\n",
    "for index, row in train_df.iterrows():\n",
    "    content = row[\"content\"] if pd.notnull(row[\"content\"]) else \"\"\n",
    "\n",
    "    cleaned_content = clean_text(content)\n",
    "\n",
    "    cleaned_data.append({\"content\": cleaned_content, \"label\": row[\"label\"]})\n",
    "\n",
    "    progress.value += 1\n",
    "\n",
    "cleaned_train_df = pd.DataFrame(cleaned_data)\n",
    "cleaned_train_df = handle_missing_data(cleaned_train_df)\n",
    "\n",
    "cleaned_train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🥀 Data Augmntation\n",
    "\n",
    "Data augmentation is a technique used to increase the size of the dataset by adding slightly modified copies of the data. This is done to improve the performance of the model by providing more data for training.\n",
    "\n",
    "Jason Wei, Kai Zou, \"EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks\"\n",
    "[Available here](https://arxiv.org/abs/1901.11196)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synonym Replacement\n",
    "\n",
    "In this technique, we replace n words in the sentence with synonyms from WordNet. We choose a random n words from the sentence that are not stop words. We then replace each of these words with a random synonym that is also present in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word: str) -> list:\n",
    "    \"\"\"\n",
    "    Get synonyms for a given word using WordNet.\n",
    "\n",
    "    :param word: Input word\n",
    "    :return: List of synonyms\n",
    "    \"\"\"\n",
    "\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():  # type: ignore\n",
    "            synonyms.add(lemma.name())\n",
    "    if word in synonyms:\n",
    "        synonyms.remove(word)\n",
    "    return list(synonyms)\n",
    "\n",
    "\n",
    "def synonym_replacement(text: str, n: int) -> str:\n",
    "    \"\"\"\n",
    "    Randomly replace words in the text with their synonyms.\n",
    "\n",
    "    :param text: Input text\n",
    "    :param n: Number of words to replace\n",
    "    :return: Text with words replaced by synonyms\n",
    "    \"\"\"\n",
    "\n",
    "    words = text.split()\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words if wordnet.synsets(word)]))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(synonyms)\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n:\n",
    "            break\n",
    "\n",
    "    sentence = \" \".join(new_words)\n",
    "\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Insertion\n",
    "\n",
    "In random insertion, we randomly insert synonyms of a word into the sentence n times. This is done by first finding a synonym of a word and then inserting it into the sentence at a random position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_word(word: str, sentence: str) -> str:\n",
    "    \"\"\"\n",
    "    Add a word at a random position in the sentence.\n",
    "\n",
    "    :param word: Word to add\n",
    "    :param sentence: Original sentence\n",
    "    :return: Sentence with the word added\n",
    "    \"\"\"\n",
    "\n",
    "    words = sentence.split()\n",
    "    random_idx = random.randint(0, len(words) - 1)\n",
    "    words.insert(random_idx, word)\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "def random_insertion(text: str, n: int) -> str:\n",
    "    \"\"\"\n",
    "    Randomly insert words into the text.\n",
    "\n",
    "    :param text: Input text\n",
    "    :param n: Number of words to insert\n",
    "    :return: Text with words inserted\n",
    "    \"\"\"\n",
    "\n",
    "    words = text.split()\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        random_word = random.choice(words)\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if synonyms:\n",
    "            synonym = random.choice(synonyms)\n",
    "            new_words = add_word(synonym, \" \".join(new_words)).split()\n",
    "\n",
    "    return \" \".join(new_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Swap\n",
    "\n",
    "In random swap, we randomly swap two words in the sentence n times. This is done by randomly choosing two words in the sentence and then swapping their positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_swap(text: str, n: int) -> str:\n",
    "    \"\"\"\n",
    "    Randomly swap words in the text.\n",
    "\n",
    "    :param text: Input text\n",
    "    :param n: Number of words to swap\n",
    "    :return: Text with words swapped\n",
    "    \"\"\"\n",
    "\n",
    "    words = text.split()\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        idx1, idx2 = random.sample(range(len(words)), 2)\n",
    "        new_words[idx1], new_words[idx2] = new_words[idx2], new_words[idx1]\n",
    "\n",
    "    return \" \".join(new_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Deletion\n",
    "\n",
    "In random deletion, we randomly delete each word in the sentence with a probability p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_deletion(text: str, p: float) -> str:\n",
    "    \"\"\"\n",
    "    Randomly delete words from the text with a given probability.\n",
    "\n",
    "    :param text: Input text\n",
    "    :param p: Probability of deletion\n",
    "    :return: Text with words deleted\n",
    "    \"\"\"\n",
    "\n",
    "    words = text.split()\n",
    "    if len(words) == 1:\n",
    "        return text\n",
    "\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        r = random.uniform(0, 1)\n",
    "        if r > p:\n",
    "            new_words.append(word)\n",
    "\n",
    "    if len(new_words) == 0:\n",
    "        return random.choice(words)\n",
    "\n",
    "    return \" \".join(new_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonymReplaced_data = []\n",
    "randomInserted_data = []\n",
    "randomSwapped_data = []\n",
    "randomDeleted_data = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress = widgets.IntProgress(\n",
    "    value=0, min=0, max=len(cleaned_train_df), description=\"Progress:\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(progress)\n",
    "\n",
    "for index, row in cleaned_train_df.iterrows():\n",
    "    content = row[\"content\"] if pd.notnull(row[\"content\"]) else row[\"content\"]\n",
    "\n",
    "    try:\n",
    "        if pd.notnull(content):\n",
    "            try:\n",
    "                synonymReplaced_content = synonym_replacement(\n",
    "                    content, (len(content.split()) // 3)\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Synonym replacement error for content: {e}\")\n",
    "                synonymReplaced_content = content\n",
    "\n",
    "            try:\n",
    "                randomInserted_content = random_insertion(\n",
    "                    content, (len(content.split()) // 3)\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Random insertion error for content: {e}\")\n",
    "                randomInserted_content = content\n",
    "\n",
    "            try:\n",
    "                randomSwapped_content = random_swap(\n",
    "                    content, (len(content.split()) // 3)\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Random swap error for content: {e}\")\n",
    "                randomSwapped_content = content\n",
    "\n",
    "            try:\n",
    "                randomDeleted_content = random_deletion(content, 0.1)\n",
    "            except Exception as e:\n",
    "                print(f\"Random deletion error for content: {e}\")\n",
    "                randomDeleted_content = content\n",
    "        else:\n",
    "            synonymReplaced_content = content\n",
    "            randomInserted_content = content\n",
    "            randomSwapped_content = content\n",
    "            randomDeleted_content = content\n",
    "    except Exception as e:\n",
    "        print(f\"Augmentation error for content: {e}\")\n",
    "        synonymReplaced_content = content\n",
    "        randomInserted_content = content\n",
    "        randomSwapped_content = content\n",
    "        randomDeleted_content = content\n",
    "\n",
    "    synonymReplaced_data.append(\n",
    "        {\n",
    "            \"content\": synonymReplaced_content,\n",
    "            \"label\": row[\"label\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    randomInserted_data.append(\n",
    "        {\n",
    "            \"content\": randomInserted_content,\n",
    "            \"label\": row[\"label\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    randomSwapped_data.append(\n",
    "        {\n",
    "            \"content\": randomSwapped_content,\n",
    "            \"label\": row[\"label\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    randomDeleted_data.append(\n",
    "        {\n",
    "            \"content\": randomDeleted_content,\n",
    "            \"label\": row[\"label\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    progress.value += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonymReplaced_df = pd.DataFrame(synonymReplaced_data)\n",
    "randomInserted_df = pd.DataFrame(randomInserted_data)\n",
    "randomSwapped_df = pd.DataFrame(randomSwapped_data)\n",
    "randomDeleted_df = pd.DataFrame(randomDeleted_data)\n",
    "\n",
    "augmented_df = pd.concat(\n",
    "    [synonymReplaced_df, randomInserted_df, randomSwapped_df, randomDeleted_df]\n",
    ")\n",
    "\n",
    "final_train_df = (\n",
    "    pd.concat([cleaned_train_df, augmented_df]).sample(frac=1).reset_index(drop=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Final training set size: {final_train_df.shape}\")\n",
    "final_train_df.to_csv(\"dataset_and_corpora/augmented_train.csv\")\n",
    "final_train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎭 Sentiment Analysis\n",
    "\n",
    "TextBlob is a Python library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "def analyze_sentiment(text: str) -> TextBlob.sentiment:  # type: ignore\n",
    "    \"\"\"\n",
    "    Analyze the sentiment of the given text using TextBlob.\n",
    "\n",
    "    :param text: Input text\n",
    "    :return: Sentiment analysis result\n",
    "    \"\"\"\n",
    "\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentiment(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add sentiment analysis results (polarity and subjectivity) to the DataFrame.\n",
    "\n",
    "    :param df: DataFrame to process\n",
    "    :return: DataFrame with sentiment analysis results\n",
    "    \"\"\"\n",
    "\n",
    "    progress = widgets.IntProgress(value=0, min=0, max=len(df), description=\"Progress:\")\n",
    "    display(progress)\n",
    "\n",
    "    polarity = []\n",
    "    subjectivity = []\n",
    "\n",
    "    for _index, row in df.iterrows():\n",
    "        sentiment = analyze_sentiment(row[\"content\"])\n",
    "        polarity.append(sentiment.polarity)\n",
    "        subjectivity.append(sentiment.subjectivity)\n",
    "        progress.value += 1\n",
    "\n",
    "    df[\"polarity\"] = polarity\n",
    "    df[\"subjectivity\"] = subjectivity\n",
    "    df = df[[\"content\", \"polarity\", \"subjectivity\", \"label\"]]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_df = add_sentiment(final_train_df)\n",
    "final_train_df.to_csv(\"dataset_and_corpora/augmented_train_senti.csv\", index=False)\n",
    "final_train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔢 Feature Extraction\n",
    "\n",
    "Extracting features for model training\n",
    "- TF-IDF (for SVM)\n",
    "- Word2Vec + GloVe (for RF and Gradient Boosting)\n",
    "- SmallBERT (for LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_df = pd.read_csv(\"dataset_and_corpora/augmented_train_senti.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Feature Extarction\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical measure used in natural language processing and information retrieval to evaluate the importance of a word in a document relative to a collection of documents (corpus).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tfidf_features(\n",
    "    df: pd.DataFrame, max_features: int = 5000\n",
    ") -> sp.sparse.csr.csr_matrix:\n",
    "    \"\"\"\n",
    "    Extract TF-IDF features from the 'content' column of the DataFrame.\n",
    "\n",
    "    :param df: Input DataFrame with a 'content' column\n",
    "    :param max_features: Maximum number of features to extract\n",
    "    :return: TF-IDF features as a sparse matrix\n",
    "    \"\"\"\n",
    "\n",
    "    progress = widgets.IntProgress(value=0, min=0, max=1, description=\"TF-IDF:\")\n",
    "    display(progress)\n",
    "\n",
    "    df[\"content\"] = df[\"content\"].fillna(\"\")\n",
    "    vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "    tfidf_features = vectorizer.fit_transform(df[\"content\"])\n",
    "\n",
    "    progress.value = 1\n",
    "    return tfidf_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe Feature Extraction\n",
    "\n",
    "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n",
    "\n",
    "**Introduced in** Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf).\n",
    "\n",
    "Before running the cell, please download and move the pre-trained word vectors (Wikipedia 2014 + Gigaword 5) from [here](https://nlp.stanford.edu/data/glove.6B.zip) to the `datasets_and_corpora` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_glove_features(\n",
    "    df: pd.DataFrame,\n",
    "    glove_path: str = \"dataset_and_corpora/glove.6B.100d.txt\",\n",
    "    embedding_dim: int = 100,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Extract GloVe features from the 'content' column of the DataFrame.\n",
    "\n",
    "    :param df: Input DataFrame with a 'content' column\n",
    "    :param glove_path: Path to the GloVe embeddings file\n",
    "    :param embedding_dim: Dimension of the GloVe embeddings\n",
    "    :return: GloVe features as a NumPy array\n",
    "    \"\"\"\n",
    "    progress = widgets.IntProgress(value=0, min=0, max=len(df), description=\"GloVe:\")\n",
    "    display(progress)\n",
    "\n",
    "    # Load GloVe embeddings\n",
    "    glove_embeddings = {}\n",
    "    with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "            glove_embeddings[word] = vector\n",
    "\n",
    "    # Compute sentence embeddings\n",
    "    sentences = [content.split() for content in df[\"content\"]]\n",
    "    glove_features = np.array(\n",
    "        [\n",
    "            np.mean(\n",
    "                [\n",
    "                    glove_embeddings[word]\n",
    "                    for word in sentence\n",
    "                    if word in glove_embeddings\n",
    "                ]\n",
    "                or [np.zeros(embedding_dim)],\n",
    "                axis=0,\n",
    "            )\n",
    "            for sentence in sentences\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for _ in range(len(df)):\n",
    "        progress.value += 1\n",
    "\n",
    "    return glove_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DistilBERT Feature Extraction\n",
    "\n",
    "[DistilBERT](https://huggingface.co/docs/transformers/en/model_doc/distilbert) is pretrained by knowledge distillation to create a smaller model with faster inference and requires less compute to train. Through a triple loss objective during pretraining, language modeling loss, distillation loss, cosine-distance loss, DistilBERT demonstrates similar performance to a larger transformer language model.\n",
    "\n",
    "**Introduced in** Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bert_features(\n",
    "    df: pd.DataFrame,\n",
    "    model_name: str = \"distilbert-base-uncased\",\n",
    "    chunk_size: int = 10,\n",
    "    output_file: str = \"features/distilbert_vectorizer.pkl\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Extract BERT features from the 'content' column of the DataFrame.\n",
    "\n",
    "    :param df: Input DataFrame with a 'content' column\n",
    "    :param model_name: Name of the pre-trained BERT model\n",
    "    :param chunk_size: Number of samples to process at once\n",
    "    :param output_file: Path to save the extracted features\n",
    "    :return: Path to the saved features file\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "    model = DistilBertModel.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "    progress = widgets.IntProgress(value=0, min=0, max=len(df), description=\"BERT:\")\n",
    "    display(progress)\n",
    "\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        for i in range(0, len(df), chunk_size):\n",
    "            chunk = df.iloc[i : i + chunk_size]\n",
    "            chunk_features = []\n",
    "\n",
    "            outputs = None\n",
    "            inputs = None\n",
    "\n",
    "            for content in chunk[\"content\"]:\n",
    "                inputs = tokenizer(\n",
    "                    content,\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                    padding=True,\n",
    "                    max_length=512,\n",
    "                )\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**inputs)\n",
    "                cls_embedding = (\n",
    "                    outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "                )\n",
    "                chunk_features.append(cls_embedding)\n",
    "                progress.value += 1\n",
    "\n",
    "            # Save chunk features to pickle file\n",
    "            pickle.dump(chunk_features, f)\n",
    "            del inputs, outputs, chunk_features\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Features saved to {output_file}\")\n",
    "    return output_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_features = extract_tfidf_features(final_train_df)\n",
    "with open(\"features/tfidf_vectorizer.pkl\", \"wb\") as file:\n",
    "    pickle.dump(tfidf_features, file)\n",
    "del tfidf_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_features = extract_glove_features(final_train_df, glove_path=\"glove.6B.100d.txt\")\n",
    "with open(\"features/glove_vectorizer.pkl\", \"wb\") as file:\n",
    "    pickle.dump(glove_features, file)\n",
    "del glove_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f0554cc4554773b544346fda4b15b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='BERT:', max=285640)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m bert_features = \u001b[43mextract_bert_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_train_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mdataset_and_corpora/distilbert_vectorizer.pkl\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m      3\u001b[39m     pickle.dump(bert_features, file)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mextract_bert_features\u001b[39m\u001b[34m(df, model_name, chunk_size, output_file)\u001b[39m\n\u001b[32m     23\u001b[39m inputs = tokenizer(\n\u001b[32m     24\u001b[39m     content,\n\u001b[32m     25\u001b[39m     return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m     max_length=\u001b[32m512\u001b[39m,\n\u001b[32m     29\u001b[39m )\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m cls_embedding = (\n\u001b[32m     33\u001b[39m     outputs.last_hidden_state[:, \u001b[32m0\u001b[39m, :].squeeze().cpu().numpy()\n\u001b[32m     34\u001b[39m )\n\u001b[32m     35\u001b[39m chunk_features.append(cls_embedding)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\goofy\\Cache\\Conda\\envs\\proj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\goofy\\Cache\\Conda\\envs\\proj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\goofy\\Cache\\Conda\\envs\\proj\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:796\u001b[39m, in \u001b[36mDistilBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._use_sdpa \u001b[38;5;129;01mand\u001b[39;00m head_mask_is_none \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[32m    792\u001b[39m         attention_mask = _prepare_4d_attention_mask_for_sdpa(\n\u001b[32m    793\u001b[39m             attention_mask, embeddings.dtype, tgt_len=input_shape[\u001b[32m1\u001b[39m]\n\u001b[32m    794\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m796\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\goofy\\Cache\\Conda\\envs\\proj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\goofy\\Cache\\Conda\\envs\\proj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\goofy\\Cache\\Conda\\envs\\proj\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:549\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    541\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    542\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    543\u001b[39m         hidden_state,\n\u001b[32m   (...)\u001b[39m\u001b[32m    546\u001b[39m         output_attentions,\n\u001b[32m    547\u001b[39m     )\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    556\u001b[39m hidden_state = layer_outputs[-\u001b[32m1\u001b[39m]\n\u001b[32m    558\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\goofy\\Cache\\Conda\\envs\\proj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\goofy\\Cache\\Conda\\envs\\proj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\goofy\\Cache\\Conda\\envs\\proj\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:475\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, x, attn_mask, head_mask, output_attentions)\u001b[39m\n\u001b[32m    465\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    466\u001b[39m \u001b[33;03mParameters:\u001b[39;00m\n\u001b[32m    467\u001b[39m \u001b[33;03m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    472\u001b[39m \u001b[33;03m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[32m    473\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[38;5;66;03m# Self-Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m sa_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[32m    484\u001b[39m     sa_output, sa_weights = sa_output  \u001b[38;5;66;03m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\goofy\\Cache\\Conda\\envs\\proj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\goofy\\Cache\\Conda\\envs\\proj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\goofy\\Cache\\Conda\\envs\\proj\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:401\u001b[39m, in \u001b[36mDistilBertSdpaAttention.forward\u001b[39m\u001b[34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[39m\n\u001b[32m    398\u001b[39m     k = k.contiguous()\n\u001b[32m    399\u001b[39m     v = v.contiguous()\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m attn_output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m    \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m attn_output = unshape(attn_output)\n\u001b[32m    411\u001b[39m attn_output = \u001b[38;5;28mself\u001b[39m.out_lin(attn_output)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "opfile = extract_bert_features(final_train_df)\n",
    "print(f\"Features saved to {opfile}\\n\")\n",
    "# The function already saves the features to a file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"dataset/tfidf_vectorizer.pkl\", \"rb\") as file:\n",
    "#     loaded_vectorizer_pickle = pickle.load(file)\n",
    "# with open(\"dataset/GloVe.pkl\", \"rb\") as file:\n",
    "#     loaded_glove_pickle = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📱 Display Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayRandomSample(df: pd.DataFrame, dataType: int) -> None:\n",
    "    \"\"\"\n",
    "    Display a random sample from the DataFrame.\n",
    "\n",
    "    :param df: DataFrame to sample from\n",
    "    :param dataType: Type of DataFrame to sample from (0 for original, 1 for augmented)\n",
    "    \"\"\"\n",
    "\n",
    "    if dataType == 0:\n",
    "        sample = df.sample(n=1)\n",
    "        print(f\"idx: {sample.index[0]}\")\n",
    "        text = sample.iloc[0][\"text\"]\n",
    "        if isinstance(text, str):\n",
    "            text = f\"{text[:200]}...\"\n",
    "        else:\n",
    "            text = \"N/A\"\n",
    "        display(\n",
    "            HTML(\n",
    "                f\"<b>Original:</b><br>\"\n",
    "                f\"<b>Title:</b> {sample.iloc[0]['title']}<br>\"\n",
    "                f\"<b>Text:</b> {text}<br>\"\n",
    "                f\"<b>Label:</b> {sample.iloc[0]['label']}<br>\"\n",
    "            )\n",
    "        )\n",
    "    if dataType == 1:\n",
    "        sample = df.sample(n=1)\n",
    "        print(f\"idx: {sample.index[0]}\")\n",
    "        text = sample.iloc[0][\"content\"]\n",
    "        if isinstance(text, str):\n",
    "            text = f\"{text[:200]}...\"\n",
    "        else:\n",
    "            text = \"N/A\"\n",
    "        display(\n",
    "            HTML(\n",
    "                f\"<b>Augmented:</b><br>\"\n",
    "                f\"<b>Content:</b> {text}<br>\"\n",
    "                f\"<b>Polarity:</b> {sample.iloc[0]['polarity']}<br>\"\n",
    "                f\"<b>Subjectivity:</b> {sample.iloc[0]['subjectivity']}<br>\"\n",
    "                f\"<b>Label:</b> {sample.iloc[0]['label']}\"\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayRandomSample(final_train_df, 1)\n",
    "print(final_train_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 🧠 Model Building\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing 5 models for ensemble learning model:\n",
    "1. Random Forest (RF)\n",
    "2. Support Vector Machine (SVM)\n",
    "3. Gradient Boosting (XGBoost/LightGBM)\n",
    "4. Logistic Regression (LR)\n",
    "5. Small BERT (DistilBERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Load GloVe features and labels\n",
    "with open(\"features/glove_vectorizer.pkl\", \"rb\") as file:\n",
    "    glove_features = pickle.load(file)\n",
    "\n",
    "labels = final_train_df[\"label\"].to_numpy()\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "glove_features_tensor = torch.tensor(glove_features, dtype=torch.float32)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Initialize k-fold cross-validation\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize variables to store results\n",
    "fold_accuracies = []\n",
    "ensemble_predictions = np.zeros((len(labels), len(np.unique(labels))))\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(glove_features)):\n",
    "    print(f\"Fold {fold + 1}/{k}\")\n",
    "\n",
    "    # Split the data into training and testing sets for this fold\n",
    "    X_train, X_test = glove_features_tensor[train_idx], glove_features_tensor[test_idx]\n",
    "    y_train, y_test = labels_tensor[train_idx], labels_tensor[test_idx]\n",
    "\n",
    "    # Create DataLoader for training and testing\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize the Random Forest model (using sklearn for simplicity)\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "    # Train the model\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        rf_model.fit(X_batch.numpy(), y_batch.numpy())\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = []\n",
    "    y_pred_proba = []\n",
    "    for X_batch, _ in test_loader:\n",
    "        preds = rf_model.predict(X_batch.numpy())\n",
    "        preds_proba = rf_model.predict_proba(X_batch.numpy())\n",
    "        y_pred.extend(preds)\n",
    "        y_pred_proba.extend(preds_proba)\n",
    "\n",
    "    # Store predictions for ensemble learning\n",
    "    ensemble_predictions[test_idx] = np.array(y_pred_proba)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test.numpy(), y_pred)\n",
    "    fold_accuracies.append(accuracy)\n",
    "    print(f\"Accuracy for fold {fold + 1}: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test.numpy(), y_pred))\n",
    "\n",
    "# Calculate and print the average accuracy across all folds\n",
    "average_accuracy = np.mean(fold_accuracies)\n",
    "print(f\"\\nAverage Accuracy across {k} folds: {average_accuracy:.4f}\")\n",
    "\n",
    "# Save the ensemble predictions for later use\n",
    "with open(\"features/rf_ensemble_predictions.pkl\", \"wb\") as file:\n",
    "    pickle.dump(ensemble_predictions, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "\n",
    "yes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load TF-IDF features and labels\n",
    "with open(\"features/tfidf_vectorizer.pkl\", \"rb\") as file:\n",
    "    tfidf_features = pickle.load(file)\n",
    "\n",
    "labels = final_train_df[\"label\"].to_numpy()\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "tfidf_features = torch.tensor(tfidf_features.toarray(), dtype=torch.float32)\n",
    "labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Define the SVM model\n",
    "class SVMModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SVMModel, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, 2)  # Binary classification (2 classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# Initialize k-fold cross-validation\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize variables to store results\n",
    "fold_accuracies = []\n",
    "ensemble_predictions = torch.zeros((len(labels), 2), dtype=torch.float32)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(tfidf_features.numpy())):\n",
    "    print(f\"Fold {fold + 1}/{k}\")\n",
    "\n",
    "    # Split the data into training and testing sets for this fold\n",
    "    X_train, X_test = tfidf_features[train_idx], tfidf_features[test_idx]\n",
    "    y_train, y_test = labels[train_idx], labels[test_idx]\n",
    "\n",
    "    # Create DataLoaders for batch processing\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    model = SVMModel(input_dim=X_train.shape[1]).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train the model\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Save the model after each epoch\n",
    "        torch.save(\n",
    "            model.state_dict(), f\"models/svm_model_fold{fold + 1}_epoch{epoch + 1}.pth\"\n",
    "        )\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    all_outputs = []\n",
    "    all_y_test = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            all_outputs.append(outputs)\n",
    "            all_y_test.append(batch_y)\n",
    "\n",
    "    # Concatenate outputs and labels\n",
    "    outputs = torch.cat(all_outputs, dim=0)\n",
    "    y_test = torch.cat(all_y_test, dim=0)\n",
    "    _, y_pred = torch.max(outputs, 1)\n",
    "    y_pred_proba = torch.softmax(outputs, dim=1)\n",
    "\n",
    "    # Store predictions for ensemble learning\n",
    "    ensemble_predictions[test_idx] = y_pred_proba\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test.cpu(), y_pred.cpu())\n",
    "    fold_accuracies.append(accuracy)\n",
    "    print(f\"Accuracy for fold {fold + 1}: {accuracy:.4f}\")\n",
    "    print(\n",
    "        \"\\nClassification Report:\\n\", classification_report(y_test.cpu(), y_pred.cpu())\n",
    "    )\n",
    "\n",
    "# Calculate and print the average accuracy across all folds\n",
    "average_accuracy = sum(fold_accuracies) / len(fold_accuracies)\n",
    "print(f\"\\nAverage Accuracy across {k} folds: {average_accuracy:.4f}\")\n",
    "\n",
    "# Save the ensemble predictions for later use\n",
    "with open(\"features/svm_ensemble_predictions.pkl\", \"wb\") as file:\n",
    "    pickle.dump(ensemble_predictions.cpu().numpy(), file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load DistilBERT features and labels\n",
    "with open(\"features/distilbert_vectorizer.pkl\", \"rb\") as file:\n",
    "    distilbert_features = pickle.load(file)\n",
    "\n",
    "labels = final_train_df[\"label\"].to_numpy()\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "distilbert_features = torch.tensor(distilbert_features, dtype=torch.float32)\n",
    "labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Define the Logistic Regression model\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, 2)  # Binary classification (2 classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# Initialize k-fold cross-validation\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize variables to store results\n",
    "fold_accuracies = []\n",
    "ensemble_predictions = torch.zeros((len(labels), 2), dtype=torch.float32)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(distilbert_features.numpy())):\n",
    "    print(f\"Fold {fold + 1}/{k}\")\n",
    "\n",
    "    # Split the data into training and testing sets for this fold\n",
    "    X_train, X_test = distilbert_features[train_idx], distilbert_features[test_idx]\n",
    "    y_train, y_test = labels[train_idx], labels[test_idx]\n",
    "\n",
    "    # Create DataLoaders for batch processing\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    model = LogisticRegressionModel(input_dim=X_train.shape[1]).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train the model\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Save the model after each epoch\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            f\"models/logistic_regression_fold{fold + 1}_epoch{epoch + 1}.pth\",\n",
    "        )\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    all_outputs = []\n",
    "    all_y_test = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            all_outputs.append(outputs)\n",
    "            all_y_test.append(batch_y)\n",
    "\n",
    "    # Concatenate outputs and labels\n",
    "    outputs = torch.cat(all_outputs, dim=0)\n",
    "    y_test = torch.cat(all_y_test, dim=0)\n",
    "    _, y_pred = torch.max(outputs, 1)\n",
    "    y_pred_proba = torch.softmax(outputs, dim=1)\n",
    "\n",
    "    # Store predictions for ensemble learning\n",
    "    ensemble_predictions[test_idx] = y_pred_proba\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test.cpu(), y_pred.cpu())\n",
    "    fold_accuracies.append(accuracy)\n",
    "    print(f\"Accuracy for fold {fold + 1}: {accuracy:.4f}\")\n",
    "    print(\n",
    "        \"\\nClassification Report:\\n\", classification_report(y_test.cpu(), y_pred.cpu())\n",
    "    )\n",
    "\n",
    "# Calculate and print the average accuracy across all folds\n",
    "average_accuracy = sum(fold_accuracies) / len(fold_accuracies)\n",
    "print(f\"\\nAverage Accuracy across {k} folds: {average_accuracy:.4f}\")\n",
    "\n",
    "# Save the ensemble predictions for later use\n",
    "with open(\"features/logistic_regression_ensemble_predictions.pkl\", \"wb\") as file:\n",
    "    pickle.dump(ensemble_predictions.cpu().numpy(), file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smallbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Initialize k-fold cross-validation\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize variables to store results\n",
    "fold_accuracies = []\n",
    "ensemble_predictions = torch.zeros((len(labels), 2), dtype=torch.float32)\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(distilbert_features.numpy())):\n",
    "    print(f\"Fold {fold + 1}/{k}\")\n",
    "\n",
    "    # Split the data into training and testing sets for this fold\n",
    "    X_train, X_test = distilbert_features[train_idx], distilbert_features[test_idx]\n",
    "    y_train, y_test = labels[train_idx], labels[test_idx]\n",
    "\n",
    "    # Create DataLoaders for batch processing\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
    "    test_dataset = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    # Train the model\n",
    "    num_epochs = 3\n",
    "    gradient_accumulation_steps = 2\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        for step, (batch_X, batch_y) in enumerate(train_loader):\n",
    "            # Tokenize the input and move to the GPU\n",
    "            batch_X = tokenizer(\n",
    "                batch_X.tolist(), padding=True, truncation=True, return_tensors=\"pt\"\n",
    "            )\n",
    "            input_ids = batch_X[\"input_ids\"].to(device)\n",
    "            attention_mask = batch_X[\"attention_mask\"].to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, labels=batch_y\n",
    "            )\n",
    "            loss = outputs.loss / gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    all_outputs = []\n",
    "    all_y_test = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            # Tokenize the input and move to the GPU\n",
    "            batch_X = tokenizer(\n",
    "                batch_X.tolist(), padding=True, truncation=True, return_tensors=\"pt\"\n",
    "            )\n",
    "            input_ids = batch_X[\"input_ids\"].to(device)\n",
    "            attention_mask = batch_X[\"attention_mask\"].to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            all_outputs.append(outputs.logits)\n",
    "            all_y_test.append(batch_y)\n",
    "\n",
    "    # Concatenate outputs and labels\n",
    "    outputs = torch.cat(all_outputs, dim=0)\n",
    "    y_test = torch.cat(all_y_test, dim=0)\n",
    "    _, y_pred = torch.max(outputs, 1)\n",
    "    y_pred_proba = torch.softmax(outputs, dim=1)\n",
    "\n",
    "    # Store predictions for ensemble learning\n",
    "    ensemble_predictions[test_idx] = y_pred_proba\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test.cpu(), y_pred.cpu())\n",
    "    fold_accuracies.append(accuracy)\n",
    "    print(f\"Accuracy for fold {fold + 1}: {accuracy:.4f}\")\n",
    "    print(\n",
    "        \"\\nClassification Report:\\n\", classification_report(y_test.cpu(), y_pred.cpu())\n",
    "    )\n",
    "\n",
    "# Calculate and print the average accuracy across all folds\n",
    "average_accuracy = sum(fold_accuracies) / len(fold_accuracies)\n",
    "print(f\"\\nAverage Accuracy across {k} folds: {average_accuracy:.4f}\")\n",
    "\n",
    "# Save the ensemble predictions for later use\n",
    "with open(\"features/smallbert_ensemble_predictions.pkl\", \"wb\") as file:\n",
    "    pickle.dump(ensemble_predictions.cpu().numpy(), file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
