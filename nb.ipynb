{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“° Fake News Detection Using Ensemble Learning\n",
    "\n",
    "---\n",
    "\n",
    "| S. No |    ID No.     | Name             |\n",
    "| ----: | :-----------: | :--------------- |\n",
    "|    1. | 2022A7PS0003U | Yusra Hakim      |\n",
    "|    2. | 2022A7PS0019U | Joseph Cijo      |\n",
    "|    3. | 2022A7PS0031U | Ritvik Bhatnagar |\n",
    "\n",
    "This Jupyter Notebook is for the project in Data Mining (CS F415) course. It contains the code used for preparing the ensemble model using the [dataset](#dataset-used) mentioned below.\n",
    "\n",
    "### Dataset Details\n",
    "\n",
    "**Dataset introduced in:**\n",
    "V. Pawan Kumar, A. Prateek, A. Ivone and P. Radu, \"WELFake: Word Embedding Over Linguistic Features for Fake News Detection,\" _IEEE Transactions on Computational Social Systems_, vol. 8, no. 4, pp. 881-893, 2021.\n",
    "[Kaggle | WELFake Dataset](https://www.kaggle.com/datasets/saurabhshahane/fake-news-classification)\n",
    "\n",
    "### Setting Up an Environment for the Jupyter notebook\n",
    "\n",
    "Firstly install Miniconda from [here](https://docs.anaconda.com/miniconda/install/).\n",
    "\n",
    "Then open a command prompt in this directory, and run the following. This will create and activate an environment called \"PROJ\".\n",
    "\n",
    "```bash\n",
    "    conda create -n proj python=3.12\n",
    "    conda activate proj\n",
    "```\n",
    "\n",
    "After running this, your CMD prompt should have a \"`(proj)`\" prefixed at the start.\n",
    "\n",
    "Run the following command to install packages, such as [PyTorch](https://pytorch.org/get-started/locally/). This will take some time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%conda install -n proj ipykernel ipywidgets --update-deps --force-reinstall\n",
    "%conda install -n proj nltk\n",
    "%conda install -n proj conda-forge::textblob\n",
    "%pip install scikit-learn matplotlib pandas pyperclip contractions scipy numpy\n",
    "%conda install -n proj conda-forge::transformers\n",
    "%pip install torch --index-url https://download.pytorch.org/whl/cu124\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to select the PROJ environment at the bottom-right.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ðŸ“š Outline\n",
    "\n",
    "To train the model, the following steps must be followed:\n",
    "\n",
    "<img src=\"archdiag.png\" size=64/>\n",
    "\n",
    "The model will accept two inputs:\n",
    "1. The headline of the article\n",
    "2. Content of the article (Optional)\n",
    "\n",
    "And give an output if it is fake news or factual (real) news.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ðŸ“‘ Dataset Tomfoolery\n",
    "\n",
    "The dataset is a CSV file with the following columns:\n",
    "- title: the title of the article\n",
    "- text: the text of the article\n",
    "- label: the label of the article (0 for fake, 1 for real)\n",
    "\n",
    "The dataset used is the WELFake dataset [available here](https://www.kaggle.com/datasets/saurabhshahane/fake-news-classification)\n",
    "\n",
    "This dataset is a collection of news articles, with each article labeled as either fake or real. The dataset contains 10,000 articles, with 5,000 fake and 5,000 real articles. The articles are in English and cover a wide range of topics, including politics, sports, entertainment, and technology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import random\n",
    "import re\n",
    "import contractions\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers.models.distilbert import DistilBertTokenizer, DistilBertModel\n",
    "import torch\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset/WELFake_Dataset.csv\", index_col=0)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ‚ï¸ Splitting Dataset\n",
    "\n",
    "The dataset is split as\n",
    "-  80% â†’ Training + Validation Set\n",
    "-  20% â†’ Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training set size: {train_df.shape}\")\n",
    "train_df.to_csv(\"dataset/train.csv\", index=False)\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nTesting set size: {test_df.shape}\")\n",
    "test_df.to_csv(\"dataset/test.csv\", index=False)\n",
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ‘¾ Data Preprocessing\n",
    "\n",
    "The data is preprocessed using the following steps\n",
    "1. [Text Cleaning](#text-cleaning)\n",
    "   1. [Lowercasing & URL Removal](#lowercasing-url-removal)\n",
    "   2. [Contractions Expansion](#contractions-expansion)\n",
    "   3. [Tokenization](#tokenization)\n",
    "   4. [Lemmatization](#lemmatization)\n",
    "   5. [Stopword Removal](#stopword-removal)\n",
    "2. [Data Augmentaiton](#data-augmentation)\n",
    "   1. [Synonym Replacement](#synonym-replacement)\n",
    "   2. [Random Insertion](#random-insertion)\n",
    "   3. [Random Swap](#random-swap)\n",
    "   4. [Random Deletion](#random-deletion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§¹ Data Cleaning\n",
    "\n",
    "The text in the datast has to be cleaned before it can be used for training the model. The following steps are used for cleaning the text.\n",
    "\n",
    "[Kaggle | Getting started with Text Preprocessing](https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging Text Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_title_and_text(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge the 'title' and 'text' fields into a single 'content' field.\n",
    "\n",
    "    :param df: Input DataFrame\n",
    "    :return: DataFrame with a new 'content' field\n",
    "    \"\"\"\n",
    "\n",
    "    df[\"content\"] = df[\"title\"].fillna(\"\") + \" \" + df[\"text\"].fillna(\"\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Handle missing data in the DataFrame by removing rows with any empty, NaN values, or rows containing only whitespaces.\n",
    "\n",
    "    :param df: DataFrame to process\n",
    "    :return: DataFrame with rows containing any empty, NaN values, or whitespaces-only content removed\n",
    "    \"\"\"\n",
    "\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(\"Missing values in each column:\\n\", missing_values)\n",
    "\n",
    "    df = df.replace(\"\", np.nan)\n",
    "\n",
    "    df = df.dropna()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowercasing & URL Removal\n",
    "\n",
    "The text is converted to lowercase and URLs are removed from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_and_remove_urls(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert text to lowercase and remove URLs.\n",
    "\n",
    "    :param text: Input text\n",
    "    :return: Processed text with URLs removed\n",
    "    \"\"\"\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contractions Expansion\n",
    "\n",
    "Contractions are expanded to their full form. For example, \"I'm\" is expanded to \"I am\". Using the [`contractions`](https://github.com/kootenpv/contractions/tree/master) library for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Expand contractions in the text using the `contractions` library.\n",
    "\n",
    "    :param text: Input text\n",
    "    :return: Text with contractions expanded\n",
    "    \"\"\"\n",
    "\n",
    "    expanded_words = [contractions.fix(word) for word in text.split()]\n",
    "    expanded_text = \" \".join(expanded_words)  # type: ignore\n",
    "    return expanded_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "\n",
    "The text is tokenized into words. This also removes punctuation and any special characters.\n",
    "\n",
    "The [`RegexpTokenizer`](https://www.nltk.org/api/nltk.tokenize.RegexpTokenizer.html) tokenizer is used for tokenization by aplphanumeric words and to ignore all other characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Tokenize the input text using NLTK's RegexpTokenizer.\n",
    "\n",
    "    :param text: Input text\n",
    "    :return: List of tokens\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "\n",
    "The words are lemmatized to their root form. This is done using the [`WordNetLemmatizer`](https://www.nltk.org/api/nltk.stem.html#nltk.stem.wordnet.WordNetLemmatizer) from the NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "nltk.download(\"wordnet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def nltkToWordnet(nltk_tag: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert NLTK POS tags to WordNet POS tags.\n",
    "\n",
    "    :param nltk_tag: NLTK POS tag\n",
    "    :return: WordNet POS tag\n",
    "    \"\"\"\n",
    "\n",
    "    if nltk_tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None  # type: ignore\n",
    "\n",
    "\n",
    "def lemmatize_text(tokens: list) -> list:\n",
    "    \"\"\"\n",
    "    Lemmatize the tokens using NLTK's WordNetLemmatizer.\n",
    "\n",
    "    :param tokens: List of tokens\n",
    "    :return: List of lemmatized words\n",
    "    \"\"\"\n",
    "\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    res_words = []\n",
    "    for word, tag in pos_tags:\n",
    "        tag = nltkToWordnet(tag)\n",
    "        if tag is None:\n",
    "            res_words.append(word)\n",
    "        else:\n",
    "            res_words.append(lemmatizer.lemmatize(word, tag))\n",
    "    return res_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopword Removal\n",
    "\n",
    "Stopwords are removed from the text. Stopwords are common words that do not add much meaning to the text. The [`stopwords`](https://www.nltk.org/api/nltk.corpus.reader.html#nltk.corpus.reader.wordlist.WordListCorpusReader) from the NLTK library are used for this purpose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"words\")\n",
    "nltk.download(\"stopwords\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "englishWords = set(nltk.corpus.words.words())\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens: list) -> str:\n",
    "    \"\"\"\n",
    "    Remove stop words from the list of tokens.\n",
    "\n",
    "    :param tokens: List of tokens\n",
    "    :return: List of tokens without stop words\n",
    "    \"\"\"\n",
    "\n",
    "    filtered_words = [w for w in tokens if (w in englishWords and w not in stop_words)]\n",
    "    filtered_text = \" \".join(filtered_words)\n",
    "    return filtered_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing Text Cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to perform text cleaning on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean the input text by applying various preprocessing steps.\n",
    "\n",
    "    :param text: Input text\n",
    "    :return: Cleaned text\n",
    "    \"\"\"\n",
    "    text = lowercase_and_remove_urls(text)\n",
    "    text = expand_contractions(text)\n",
    "    tokens = tokenize_text(text)\n",
    "    tokens = lemmatize_text(tokens)\n",
    "    text = remove_stopwords(tokens)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress = widgets.IntProgress(\n",
    "    value=0, min=0, max=len(train_df), description=\"Progress:\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(progress)\n",
    "\n",
    "cleaned_data = []\n",
    "train_df = merge_title_and_text(train_df)\n",
    "train_df = handle_missing_data(train_df)\n",
    "\n",
    "for index, row in train_df.iterrows():\n",
    "    content = row[\"content\"] if pd.notnull(row[\"content\"]) else \"\"\n",
    "\n",
    "    cleaned_content = clean_text(content)\n",
    "\n",
    "    cleaned_data.append({\"content\": cleaned_content, \"label\": row[\"label\"]})\n",
    "\n",
    "    progress.value += 1\n",
    "\n",
    "cleaned_train_df = pd.DataFrame(cleaned_data)\n",
    "cleaned_train_df = handle_missing_data(cleaned_train_df)\n",
    "\n",
    "cleaned_train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¥€ Data Augmntation\n",
    "\n",
    "Data augmentation is a technique used to increase the size of the dataset by adding slightly modified copies of the data. This is done to improve the performance of the model by providing more data for training.\n",
    "\n",
    "Jason Wei, Kai Zou, \"EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks\"\n",
    "[Available here](https://arxiv.org/abs/1901.11196)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synonym Replacement\n",
    "\n",
    "In this technique, we replace n words in the sentence with synonyms from WordNet. We choose a random n words from the sentence that are not stop words. We then replace each of these words with a random synonym that is also present in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word: str) -> list:\n",
    "    \"\"\"\n",
    "    Get synonyms for a given word using WordNet.\n",
    "\n",
    "    :param word: Input word\n",
    "    :return: List of synonyms\n",
    "    \"\"\"\n",
    "\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():  # type: ignore\n",
    "            synonyms.add(lemma.name())\n",
    "    if word in synonyms:\n",
    "        synonyms.remove(word)\n",
    "    return list(synonyms)\n",
    "\n",
    "\n",
    "def synonym_replacement(text: str, n: int) -> str:\n",
    "    \"\"\"\n",
    "    Randomly replace words in the text with their synonyms.\n",
    "\n",
    "    :param text: Input text\n",
    "    :param n: Number of words to replace\n",
    "    :return: Text with words replaced by synonyms\n",
    "    \"\"\"\n",
    "\n",
    "    words = text.split()\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words if wordnet.synsets(word)]))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(synonyms)\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n:\n",
    "            break\n",
    "\n",
    "    sentence = \" \".join(new_words)\n",
    "\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Insertion\n",
    "\n",
    "In random insertion, we randomly insert synonyms of a word into the sentence n times. This is done by first finding a synonym of a word and then inserting it into the sentence at a random position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_word(word: str, sentence: str) -> str:\n",
    "    \"\"\"\n",
    "    Add a word at a random position in the sentence.\n",
    "\n",
    "    :param word: Word to add\n",
    "    :param sentence: Original sentence\n",
    "    :return: Sentence with the word added\n",
    "    \"\"\"\n",
    "\n",
    "    words = sentence.split()\n",
    "    random_idx = random.randint(0, len(words) - 1)\n",
    "    words.insert(random_idx, word)\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "def random_insertion(text: str, n: int) -> str:\n",
    "    \"\"\"\n",
    "    Randomly insert words into the text.\n",
    "\n",
    "    :param text: Input text\n",
    "    :param n: Number of words to insert\n",
    "    :return: Text with words inserted\n",
    "    \"\"\"\n",
    "\n",
    "    words = text.split()\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        random_word = random.choice(words)\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if synonyms:\n",
    "            synonym = random.choice(synonyms)\n",
    "            new_words = add_word(synonym, \" \".join(new_words)).split()\n",
    "\n",
    "    return \" \".join(new_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Swap\n",
    "\n",
    "In random swap, we randomly swap two words in the sentence n times. This is done by randomly choosing two words in the sentence and then swapping their positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_swap(text: str, n: int) -> str:\n",
    "    \"\"\"\n",
    "    Randomly swap words in the text.\n",
    "\n",
    "    :param text: Input text\n",
    "    :param n: Number of words to swap\n",
    "    :return: Text with words swapped\n",
    "    \"\"\"\n",
    "\n",
    "    words = text.split()\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        idx1, idx2 = random.sample(range(len(words)), 2)\n",
    "        new_words[idx1], new_words[idx2] = new_words[idx2], new_words[idx1]\n",
    "\n",
    "    return \" \".join(new_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Deletion\n",
    "\n",
    "In random deletion, we randomly delete each word in the sentence with a probability p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_deletion(text: str, p: float) -> str:\n",
    "    \"\"\"\n",
    "    Randomly delete words from the text with a given probability.\n",
    "\n",
    "    :param text: Input text\n",
    "    :param p: Probability of deletion\n",
    "    :return: Text with words deleted\n",
    "    \"\"\"\n",
    "\n",
    "    words = text.split()\n",
    "    if len(words) == 1:\n",
    "        return text\n",
    "\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        r = random.uniform(0, 1)\n",
    "        if r > p:\n",
    "            new_words.append(word)\n",
    "\n",
    "    if len(new_words) == 0:\n",
    "        return random.choice(words)\n",
    "\n",
    "    return \" \".join(new_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonymReplaced_data = []\n",
    "randomInserted_data = []\n",
    "randomSwapped_data = []\n",
    "randomDeleted_data = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress = widgets.IntProgress(\n",
    "    value=0, min=0, max=len(cleaned_train_df), description=\"Progress:\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(progress)\n",
    "\n",
    "for index, row in cleaned_train_df.iterrows():\n",
    "    content = row[\"content\"] if pd.notnull(row[\"content\"]) else row[\"content\"]\n",
    "\n",
    "    try:\n",
    "        if pd.notnull(content):\n",
    "            try:\n",
    "                synonymReplaced_content = synonym_replacement(\n",
    "                    content, (len(content.split()) // 3)\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Synonym replacement error for content: {e}\")\n",
    "                synonymReplaced_content = content\n",
    "\n",
    "            try:\n",
    "                randomInserted_content = random_insertion(\n",
    "                    content, (len(content.split()) // 3)\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Random insertion error for content: {e}\")\n",
    "                randomInserted_content = content\n",
    "\n",
    "            try:\n",
    "                randomSwapped_content = random_swap(\n",
    "                    content, (len(content.split()) // 3)\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Random swap error for content: {e}\")\n",
    "                randomSwapped_content = content\n",
    "\n",
    "            try:\n",
    "                randomDeleted_content = random_deletion(content, 0.1)\n",
    "            except Exception as e:\n",
    "                print(f\"Random deletion error for content: {e}\")\n",
    "                randomDeleted_content = content\n",
    "        else:\n",
    "            synonymReplaced_content = content\n",
    "            randomInserted_content = content\n",
    "            randomSwapped_content = content\n",
    "            randomDeleted_content = content\n",
    "    except Exception as e:\n",
    "        print(f\"Augmentation error for content: {e}\")\n",
    "        synonymReplaced_content = content\n",
    "        randomInserted_content = content\n",
    "        randomSwapped_content = content\n",
    "        randomDeleted_content = content\n",
    "\n",
    "    synonymReplaced_data.append(\n",
    "        {\n",
    "            \"content\": synonymReplaced_content,\n",
    "            \"label\": row[\"label\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    randomInserted_data.append(\n",
    "        {\n",
    "            \"content\": randomInserted_content,\n",
    "            \"label\": row[\"label\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    randomSwapped_data.append(\n",
    "        {\n",
    "            \"content\": randomSwapped_content,\n",
    "            \"label\": row[\"label\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    randomDeleted_data.append(\n",
    "        {\n",
    "            \"content\": randomDeleted_content,\n",
    "            \"label\": row[\"label\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    progress.value += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonymReplaced_df = pd.DataFrame(synonymReplaced_data)\n",
    "randomInserted_df = pd.DataFrame(randomInserted_data)\n",
    "randomSwapped_df = pd.DataFrame(randomSwapped_data)\n",
    "randomDeleted_df = pd.DataFrame(randomDeleted_data)\n",
    "\n",
    "augmented_df = pd.concat(\n",
    "    [synonymReplaced_df, randomInserted_df, randomSwapped_df, randomDeleted_df]\n",
    ")\n",
    "\n",
    "final_train_df = (\n",
    "    pd.concat([cleaned_train_df, augmented_df]).sample(frac=1).reset_index(drop=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Final training set size: {final_train_df.shape}\")\n",
    "final_train_df.to_csv(\"dataset/augmented_train.csv\")\n",
    "final_train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŽ­ Sentiment Analysis\n",
    "\n",
    "TextBlob is a Python library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "def analyze_sentiment(text: str) -> TextBlob.sentiment:  # type: ignore\n",
    "    \"\"\"\n",
    "    Analyze the sentiment of the given text using TextBlob.\n",
    "\n",
    "    :param text: Input text\n",
    "    :return: Sentiment analysis result\n",
    "    \"\"\"\n",
    "\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentiment(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add sentiment analysis results (polarity and subjectivity) to the DataFrame.\n",
    "\n",
    "    :param df: DataFrame to process\n",
    "    :return: DataFrame with sentiment analysis results\n",
    "    \"\"\"\n",
    "\n",
    "    progress = widgets.IntProgress(value=0, min=0, max=len(df), description=\"Progress:\")\n",
    "    display(progress)\n",
    "\n",
    "    polarity = []\n",
    "    subjectivity = []\n",
    "\n",
    "    for _index, row in df.iterrows():\n",
    "        sentiment = analyze_sentiment(row[\"content\"])\n",
    "        polarity.append(sentiment.polarity)\n",
    "        subjectivity.append(sentiment.subjectivity)\n",
    "        progress.value += 1\n",
    "\n",
    "    df[\"polarity\"] = polarity\n",
    "    df[\"subjectivity\"] = subjectivity\n",
    "    df = df[[\"content\", \"polarity\", \"subjectivity\", \"label\"]]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_df = add_sentiment(final_train_df)\n",
    "final_train_df.to_csv(\"dataset/augmented_train_senti.csv\", index=False)\n",
    "final_train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¢ Feature Extraction\n",
    "\n",
    "Extracting features for model training\n",
    "- TF-IDF (for SVM)\n",
    "- Word2Vec + GloVe (for RF and Gradient Boosting)\n",
    "- SmallBERT (for LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_df = pd.read_csv(\"dataset/augmented_train_senti.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Feature Extraction\n",
    "def extract_tfidf_features(df, max_features=5000):\n",
    "    progress = widgets.IntProgress(value=0, min=0, max=1, description=\"TF-IDF:\")\n",
    "    display(progress)\n",
    "\n",
    "    # Handle missing values in the 'content' column\n",
    "    df[\"content\"] = df[\"content\"].fillna(\"\")\n",
    "    vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "    # tfidf_features = np.array(vectorizer.fit_transform(df[\"content\"]))\n",
    "    tfidf_features = vectorizer.fit_transform(df[\"content\"])\n",
    "\n",
    "    progress.value = 1\n",
    "    return tfidf_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVe Feature Extraction\n",
    "def extract_glove_features(df, glove_path=\"glove.6B.100d.txt\", embedding_dim=100):\n",
    "    progress = widgets.IntProgress(value=0, min=0, max=len(df), description=\"GloVe:\")\n",
    "    display(progress)\n",
    "\n",
    "    # Load GloVe embeddings\n",
    "    glove_embeddings = {}\n",
    "    with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "            glove_embeddings[word] = vector\n",
    "\n",
    "    # Compute sentence embeddings\n",
    "    sentences = [content.split() for content in df[\"content\"]]\n",
    "    glove_features = np.array(\n",
    "        [\n",
    "            np.mean(\n",
    "                [\n",
    "                    glove_embeddings[word]\n",
    "                    for word in sentence\n",
    "                    if word in glove_embeddings\n",
    "                ]\n",
    "                or [np.zeros(embedding_dim)],\n",
    "                axis=0,\n",
    "            )\n",
    "            for sentence in sentences\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for _ in range(len(df)):\n",
    "        progress.value += 1\n",
    "\n",
    "    return glove_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SmallBERT Feature Extraction with Chunking\n",
    "def extract_bert_features(df, model_name=\"distilbert-base-uncased\", chunk_size=100):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "    model = DistilBertModel.from_pretrained(model_name)\n",
    "    bert_features = []\n",
    "\n",
    "    progress = widgets.IntProgress(value=0, min=0, max=len(df), description=\"BERT:\")\n",
    "    display(progress)\n",
    "\n",
    "    # Process the DataFrame in chunks\n",
    "    for i in range(0, len(df), chunk_size):\n",
    "        chunk = df.iloc[i : i + chunk_size]\n",
    "        chunk_features = []\n",
    "\n",
    "        for content in chunk[\"content\"]:\n",
    "            inputs = tokenizer(\n",
    "                content,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=512,\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
    "            chunk_features.append(cls_embedding)\n",
    "            progress.value += 1\n",
    "\n",
    "        bert_features.extend(chunk_features)\n",
    "\n",
    "    return np.array(bert_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_features = extract_tfidf_features(final_train_df)\n",
    "with open(\"dataset/tfidf_vectorizer.pkl\", \"wb\") as file:\n",
    "    pickle.dump(tfidf_features, file)\n",
    "del tfidf_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_features = extract_glove_features(final_train_df, glove_path=\"glove.6B.100d.txt\")\n",
    "with open(\"dataset/glove_vectorizer.pkl\", \"wb\") as file:\n",
    "    pickle.dump(glove_features, file)\n",
    "del glove_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "572004586cbe498a8ca5400b870649fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='BERT:', max=285640)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "bert_features = extract_bert_features(final_train_df)\n",
    "with open(\"dataset/distilbert_vectorizer.pkl\", \"wb\") as file:\n",
    "    pickle.dump(bert_features, file)\n",
    "del bert_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/tfidf_vectorizer.pkl\", \"rb\") as file:\n",
    "    loaded_vectorizer_pickle = pickle.load(file)\n",
    "with open(\"dataset/GloVe.pkl\", \"rb\") as file:\n",
    "    loaded_glove_pickle = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“± Display Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayRandomSample(df: pd.DataFrame, dataType: int) -> None:\n",
    "    \"\"\"\n",
    "    Display a random sample from the DataFrame.\n",
    "\n",
    "    :param df: DataFrame to sample from\n",
    "    :param dataType: Type of DataFrame to sample from (0 for original, 1 for augmented)\n",
    "    \"\"\"\n",
    "\n",
    "    if dataType == 0:\n",
    "        sample = df.sample(n=1)\n",
    "        print(f\"idx: {sample.index[0]}\")\n",
    "        text = sample.iloc[0][\"text\"]\n",
    "        if isinstance(text, str):\n",
    "            text = f\"{text[:200]}...\"\n",
    "        else:\n",
    "            text = \"N/A\"\n",
    "        display(\n",
    "            HTML(\n",
    "                f\"<b>Original:</b><br>\"\n",
    "                f\"<b>Title:</b> {sample.iloc[0]['title']}<br>\"\n",
    "                f\"<b>Text:</b> {text}<br>\"\n",
    "                f\"<b>Label:</b> {sample.iloc[0]['label']}<br>\"\n",
    "            )\n",
    "        )\n",
    "    if dataType == 1:\n",
    "        sample = df.sample(n=1)\n",
    "        print(f\"idx: {sample.index[0]}\")\n",
    "        text = sample.iloc[0][\"content\"]\n",
    "        if isinstance(text, str):\n",
    "            text = f\"{text[:200]}...\"\n",
    "        else:\n",
    "            text = \"N/A\"\n",
    "        display(\n",
    "            HTML(\n",
    "                f\"<b>Augmented:</b><br>\"\n",
    "                f\"<b>Content:</b> {text}<br>\"\n",
    "                f\"<b>Polarity:</b> {sample.iloc[0]['polarity']}<br>\"\n",
    "                f\"<b>Subjectivity:</b> {sample.iloc[0]['subjectivity']}<br>\"\n",
    "                f\"<b>Label:</b> {sample.iloc[0]['label']}\"\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayRandomSample(final_train_df, 1)\n",
    "print(final_train_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ðŸ§  Model Building\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing 5 models for ensemble learning model:\n",
    "1. Random Forest (RF)\n",
    "2. Support Vector Machine (SVM)\n",
    "3. Gradient Boosting (XGBoost/LightGBM)\n",
    "4. Logistic Regression (LR)\n",
    "5. Small BERT (DistilBERT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
